{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어처리 과제 2 (5주차)\n",
    "* 과제는 해당 .ipynb 파일에 코드 작성\n",
    "    * 코드는 google colab의 gpu를 사용하는 런타임 환경에서 모두실행을 통해 한번에 실행 되어야함\n",
    "    * 생성형 AI (ChatGPT, Copilot, Claude, ...) 등 사용 가능\n",
    "        * 단, 사용시 사용한 방법, 입력, 출력을 캡처해 보고서에 기입\n",
    "* Word를 통해 자유형식으로 보고서를 작성\n",
    "    * 보고서의 양식은 자유\n",
    "    * 보고서의 제출은 .pdf 형식으로 제출해야하며, 파일명은 \"학번_이름_HW_??.pdf\"로 제출 할 것\n",
    "    * 보고서에 코드를 그대로 복붙 하지 말 것 (캡처 도구를 활용, 환경 설치 자료 참고)\n",
    "* .ipynb와 .pdf 파일을 el을 통해 제출\n",
    "    * 예시 : \"2232036006_임상훈_HW_01.ipynb\"와 \"2232036006_임상훈_HW_01.pdf\"를 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMDB 데이터 처리 (20점)\n",
    "\n",
    "* Stanford 대학에서 제공하는 IMDB 영화 리뷰 데이터(https://ai.stanford.edu/~amaas/data/sentiment/)를 다운 받아 학습, 테스트 데이터를 구성하시오\n",
    "\n",
    "    * 데이터는 영어 텍스트 데이터로 긍정/부정의 Binary classification 데이터셋임\n",
    "    * 데이터셋의 압축을 해제했을 떄의 각 디렉토리의 용도는 다음과 같음\n",
    "        * train/pos : 긍정 label의 학습 데이터\n",
    "        * train/neg : 부정 label의 학습 데이터\n",
    "        * test/pos : 긍정 label의 테스트 데이터\n",
    "        * test/neg : 부정 label의 테스트 데이터\n",
    "    * 지금껏 배운 다양한 기법을 적용해 tokenizing, nomalizing 등을 진행한 후 vocab을 구축하여야함\n",
    "\n",
    "**GRADING**\n",
    "* 데이터셋 전처리를 통해 vocab 구축 (+20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 통계 분석 (30점)\n",
    "* 1에서 처리한 vocab을 통해 tokenizing 된 데이터셋의 여러 통계를 계산하시오\n",
    "    * 통계의 예시\n",
    "        * 학습/테스트 문서의 수\n",
    "        * 학습/테스트 데이터의 평균 token 수\n",
    "        * 데이터의 token histogram\n",
    "        * 학습/테스트에서의 unk token의 수\n",
    "        * 각 token의 빈도 그래프\n",
    "        * 긍정/부정의 token 빈도 차이\n",
    "        * 긍정/부정의 frequent/rare token\n",
    "\n",
    "* 이전 실습까지 사용한 코드 및 검색을 활용하여 최소 1개의 그래프를 그려야 함\n",
    "\n",
    "**GRADING**\n",
    "* 분석한 통계의 수 (+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification 모델 구축 및 학습 (50점)\n",
    "* 이론 및 실습 수업을 통해 배운 MLP, CNN, RNN을 사용하여 각자의 모델을 구축하시오\n",
    "    * 모델의 크기는 ModelSummary 기준 500MB의 메모리를 초과하면 안됨\n",
    "    * 모델은 최대 10 epoch 학습 할 수 있음 (적게 학습하는 것은 ok)\n",
    "* 최대한 높은 성능을 기록하는 모델을 구축하여야 함\n",
    "    * 학습엔 주어진 학습 데이터만을 사용하여야 함\n",
    "    * 테스트 데이터를 학습에 사용하면 0점\n",
    "* 모델 구성에 있어 왜 자신이 그런 모델 구조를 설계 하였는지 설명을 하여야함\n",
    "\n",
    "**GRADING**\n",
    "* 모델 구축 및 학습 (+20)\n",
    "* 모델에 대한 설명 (+10)\n",
    "* 모델 성능에 따른 성적\n",
    "    * 상위 0~30% :  +20\n",
    "    * 상위 30~50% :  +15\n",
    "    * 상위 50~70% :  +10\n",
    "    * 상위 70~100% : +5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
