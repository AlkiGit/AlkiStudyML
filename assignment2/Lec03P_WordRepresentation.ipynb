{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pickle\n",
    "\n",
    "okt_train_file = \"okt_stem_train_dataset.pkl\"\n",
    "okt_test_file = \"okt_stem_test_dataset.pkl\"\n",
    "\n",
    "if exists(okt_train_file):\n",
    "    print(f\"{okt_train_file} already exists\")\n",
    "    with open(okt_train_file, \"rb\") as file:\n",
    "        tokenized_train_dataset = pickle.load(file)\n",
    "    with open(okt_test_file, \"rb\") as file:\n",
    "        tokenized_test_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('리뷰의 최대 길이 :',max(len(text) for text, _ in tokenized_train_dataset))\n",
    "print('리뷰의 평균 길이 :',sum(map(lambda x: len(x[0]), tokenized_train_dataset))/len(tokenized_train_dataset))\n",
    "\n",
    "plt.hist([len(text) for text, _ in tokenized_train_dataset], bins=50)\n",
    "plt.xlabel('length of text')\n",
    "plt.ylabel('number of text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kor_stopword.txt\", \"r\") as file:\n",
    "    kor_stopwords = [stopword.strip() for stopword in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_stopwords = set(kor_stopwords)  # set대신 list를 사용하면 어떻게 될까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train_datas = []\n",
    "for train_text, _ in tokenized_train_dataset:\n",
    "    word2vec_train_datas.append([word for word in train_text if word not in kor_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(text) for text in word2vec_train_datas))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, word2vec_train_datas))/len(word2vec_train_datas))\n",
    "\n",
    "plt.hist([len(text) for text in word2vec_train_datas], bins=50)\n",
    "plt.xlabel('length of text')\n",
    "plt.ylabel('number of text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "CBOW_W2V = Word2Vec(sentences = word2vec_train_datas, vector_size = 32, window = 5, min_count = 1, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(CBOW_W2V.wv.vectors))\n",
    "print(CBOW_W2V.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CBOW_W2V.wv.most_similar(\"히어로\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkipGram_W2V = Word2Vec(sentences = word2vec_train_datas, vector_size = 32, window = 5, min_count = 1, workers = 4, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(SkipGram_W2V.wv.vectors))\n",
    "print(SkipGram_W2V.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CBOW_W2V.wv.most_similar(CBOW_W2V.wv[\"공포영화\"]))\n",
    "print(CBOW_W2V.wv.most_similar(CBOW_W2V.wv[\"공포영화\"]-CBOW_W2V.wv[\"공포\"]))\n",
    "print(CBOW_W2V.wv.most_similar(CBOW_W2V.wv[\"공포영화\"]-CBOW_W2V.wv[\"공포\"]+CBOW_W2V.wv[\"액션\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SkipGram_W2V.wv.most_similar(SkipGram_W2V.wv[\"공포영화\"]))\n",
    "print(SkipGram_W2V.wv.most_similar(SkipGram_W2V.wv[\"공포영화\"]-SkipGram_W2V.wv[\"공포\"]))\n",
    "print(SkipGram_W2V.wv.most_similar(SkipGram_W2V.wv[\"공포영화\"]-SkipGram_W2V.wv[\"공포\"]+SkipGram_W2V.wv[\"액션\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_W2V.save(\"CBOW_W2V.model\")\n",
    "SkipGram_W2V.save(\"SkipGram_W2V.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkipGram_W2V = Word2Vec.load(\"SkipGram_W2V.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkipGram_W2V.wv.most_similar(CBOW_W2V.wv[\"공포영화\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counter = Counter()\n",
    "\n",
    "for tokens, _ in tokenized_train_dataset:\n",
    "    token_counter.update(tokens)\n",
    "\n",
    "min_count = 1\n",
    "vocab = {\"[PAD]\":0, \"[UNK]\":1}\n",
    "vocab_idx = 2\n",
    "\n",
    "for token, count in token_counter.items():\n",
    "    if count > min_count and token not in kor_stopwords:\n",
    "        vocab[token] = vocab_idx\n",
    "        vocab_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"공포영화\" in SkipGram_W2V.wv.key_to_index)\n",
    "print(\"[UNK]\" in SkipGram_W2V.wv.key_to_index)\n",
    "print(\"[PAD]\" in SkipGram_W2V.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SkipGram_W2V.wv.key_to_index[\"공포영화\"])\n",
    "print(vocab[\"공포영화\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "for token, idx in vocab.items():\n",
    "    if token in CBOW_W2V.wv:\n",
    "        embedding_list.append(SkipGram_W2V.wv[token])\n",
    "    elif token == \"[PAD]\":\n",
    "        embedding_list.append(np.zeros(SkipGram_W2V.wv.vectors.shape[1]))\n",
    "    elif token == \"[UNK]\":\n",
    "        embedding_list.append(np.random.uniform(-1, 1, SkipGram_W2V.wv.vectors.shape[1]))\n",
    "    else:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_loopup_matrix = np.vstack(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_loopup_matrix), freeze=False)\n",
    "        self.fc1 = nn.Linear(32 * 100, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, 32 * 100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "\n",
    "class SentimentClassifierPL(pl.LightningModule):\n",
    "    def __init__(self, sentiment_classifier):\n",
    "        super(SentimentClassifierPL, self).__init__()\n",
    "        self.model = sentiment_classifier\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.validation_step_outputs.append((loss, outputs, labels))\n",
    "        return loss, outputs, labels\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        avg_loss = torch.stack([x[0] for x in outputs]).mean()\n",
    "        self.log(\"avg_val_loss\", avg_loss)\n",
    "        \n",
    "        all_outputs = torch.cat([x[1] for x in outputs])\n",
    "        all_labels = torch.cat([x[2] for x in outputs])\n",
    "        all_preds = all_outputs.argmax(dim=1)\n",
    "        accuracy = (all_preds == all_labels).float().mean()\n",
    "        self.log(\"val_accuracy\", accuracy)\n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.test_step_outputs.append((loss, outputs, labels))\n",
    "        return loss, outputs, labels\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        outputs = self.test_step_outputs\n",
    "        avg_loss = torch.stack([x[0] for x in outputs]).mean()\n",
    "        self.log(\"avg_test_loss\", avg_loss)\n",
    "        \n",
    "        all_outputs = torch.cat([x[1] for x in outputs])\n",
    "        all_labels = torch.cat([x[2] for x in outputs])\n",
    "        all_preds = all_outputs.argmax(dim=1)\n",
    "        accuracy = (all_preds == all_labels).float().mean()\n",
    "        self.log(\"test_accuracy\", accuracy)\n",
    "        self.test_step_outputs.clear()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = int(self.data[index][1])\n",
    "        tokens = self.data[index][0]\n",
    "\n",
    "        token_ids = [self.vocab[token] if token in self.vocab else 1 for token in tokens]\n",
    "        \n",
    "        if len(token_ids) > 100:\n",
    "            token_ids = token_ids[:100]\n",
    "        else:\n",
    "            token_ids = token_ids[:100] + [0] * (100 - len(token_ids))\n",
    "            \n",
    "        return torch.tensor(token_ids), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "def check_performance(vocab,train_data, test_data, wandb_log_name):\n",
    "    wandb_logger = WandbLogger(project=\"NLP_test\", name=wandb_log_name, group=\"Lec02\")\n",
    "    \n",
    "    model = SentimentClassifier(len(vocab))\n",
    "    pl_model = SentimentClassifierPL(model)    \n",
    "    \n",
    "    train_dataset = SentimentDataset(train_data, vocab)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    val_dataset = SentimentDataset(test_data, vocab)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    test_dataset = SentimentDataset(test_data, vocab)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=1, \n",
    "                     accelerator=\"gpu\",\n",
    "                     logger=wandb_logger\n",
    "                     )\n",
    "    \n",
    "    trainer.fit(model=pl_model, \n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=val_loader)\n",
    "    \n",
    "    trainer.test(dataloaders=test_loader)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_performance(vocab, tokenized_train_dataset, tokenized_test_dataset, \"okt_stem_vocab_with_SKipGram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 문제\n",
    "아래는 사전 학습된 GloVe embedding을 다운로드하고 처리하는 코드이다.\n",
    "\n",
    "glove라는 dictionary는 단어를 key로, GloVe vector를 value로 가질 때\n",
    "\n",
    "이를 이용하여 embedding lookup matrix를 만들고 기존 사용한 모델의 embedding layer에 추가해 학습을 진행하시오.\n",
    "\n",
    "(기존에 사용한 모델의 경우 embedding의 크기가 32이므로 glove의 크기에 맞게 수정하여야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip         # https://github.com/stanfordnlp/GloVe\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "\n",
    "glove = {}\n",
    "with open(glove_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        word, *vector = line.split()\n",
    "        vector = np.array(vector)\n",
    "        glove[word] = vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
